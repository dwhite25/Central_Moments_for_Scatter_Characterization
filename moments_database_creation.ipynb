{"cells":[{"cell_type":"markdown","metadata":{"id":"usRwmEib9Ht4"},"source":["# run the initial stuff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rhpc5NJ8DE1r"},"outputs":[],"source":["import os\n","import sys\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgtXXMj4sPOA"},"outputs":[],"source":["import base_waves_class_library as bwl\n","import moments_function_library as mfl\n","import polyfit_function_library as pfl\n","import scatter_function_library as sfl\n","import gmm_library              as gmm"]},{"cell_type":"markdown","metadata":{"id":"0Sd07vkiIu_r"},"source":["# Databases\n","\n","creates both delta and GMM scatter distribution datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8h07tRbV03o"},"outputs":[],"source":["moment_names    = ['mean', 'variance', 'std', 'skew', 'kurtosis', 'inv_kurt', 'hyperskewness', 'hyperkurtosis']\n","gmm_names       = ['loc_1', 'loc_2', 'loc_3', 'amp_1', 'amp_2', 'amp_3', 'mu1', 'mu2', 'sigma', 'w1'] + moment_names\n","cols            = ['loc_1', 'loc_2', 'loc_3', 'amp_1', 'amp_2', 'amp_3'] + moment_names\n","print(cols)\n","\n","num_records     = 0\n","mid_low         = -0.1\n","mid_high        = 0.1\n","mid_step        = 0.02\n","extent_low      = 0.02\n","extent_high     = 0.401\n","extent_step     = 0.02\n","\n","# find out how long the database will need to be so we can reserve the whole array in memory up front.\n","# this will speed up the process\n","for mid in np.round(np.arange(mid_low, mid_high, mid_step), 2):\n","    for extent in np.round(np.arange(extent_low, extent_high, extent_step), 2):\n","        for amp_1 in np.round(np.arange(.05, .96, .05), 2):\n","            num_records += 1\n","for mid in np.round(np.arange(mid_low, mid_high, mid_step), 2):\n","    for extent in np.round(np.arange(extent_low, extent_high, extent_step), 2):\n","        loc_1 = np.round(mid - extent, 2)\n","        loc_3 = np.round(mid + extent, 2)\n","        for spot in np.round(np.arange(.02, extent*2-.01, .02), 2):\n","            for amp_1 in np.round(np.arange(.05, .91, .05), 2):\n","                for amp_2 in np.round(np.arange(.05, .96-amp_1, .05), 2):\n","                    num_records += 1\n","\n","print(num_records)\n","\n","SampleRate  = 1000\n","\n","x           = np.round(np.linspace(-2, 2, 4*int(SampleRate), endpoint=False), 3)\n","data_array  = np.zeros((num_records, len(cols)))\n","gmm_array   = np.zeros((num_records, len(gmm_names)))\n","\n","start       = time.time()\n","idx         = 0\n","\n","# create the database\n","# two deltas loop\n","for mid in np.round(np.arange(mid_low, mid_high, mid_step), 2):\n","    for extent in np.round(np.arange(extent_low, extent_high, extent_step), 2):\n","        loc_1 = np.round(mid - extent, 2)\n","        loc_2 = np.round(mid + extent, 2)\n","        for amp_1 in np.round(np.arange(.05, .96, .05), 2):\n","            amp_2           = np.round(1.0 - amp_1, 2)\n","            delta           = sfl.Delta(x, loc1=loc_1, loc2=loc_2, amp1=amp_1, amp2=amp_2)\n","            ems             = mfl.Moments(x, delta.scatter)\n","            vals            = [loc_1, loc_2, .49, amp_1, amp_2, 0.]\n","            data_array[idx] = np.concatenate((vals, ems.moments))\n","            target_moments  = [ems.mean, ems.variance, ems.std, ems.skew, ems.kurtosis]\n","            results, _, _   = gmm.match_moments(target_moments)\n","            gmm_scatter     = gmm.gmm_pdf(x, results)\n","            gmm_ems         = mfl.Moments(x, gmm_scatter)\n","            gmm_vals        = np.concatenate((vals, results))\n","            gmm_array[idx]  = np.concatenate((gmm_vals, gmm_ems.moments))\n","            idx += 1\n","            if idx % 1000 == 0:\n","                print(idx)\n","# three deltas loop\n","for mid in np.round(np.arange(mid_low, mid_high, mid_step), 2):\n","    for extent in np.round(np.arange(extent_low, extent_high, extent_step), 2):\n","        loc_1 = np.round(mid - extent, 2)\n","        loc_3 = np.round(mid + extent, 2)\n","        for spot in np.round(np.arange(.02, extent*2-.01, .02), 2):\n","            loc_2 = np.round(spot + loc_1, 2)\n","            for amp_1 in np.round(np.arange(.05, .91, .05), 2):\n","                for amp_2 in np.round(np.arange(.05, .96-amp_1, .05), 2):\n","                    amp_3           = np.round(1.0 - amp_1 - amp_2, 2)\n","                    delta           = sfl.Delta(x, loc1=loc_1, loc2=loc_2, loc3=loc_3,\n","                                                amp1=amp_1, amp2=amp_2, amp3=amp_3)\n","                    ems             = mfl.Moments(x, delta.scatter)\n","                    vals            = [loc_1, loc_2, loc_3, amp_1, amp_2, amp_3]\n","                    data_array[idx] = np.concatenate((vals, ems.moments))\n","                    target_moments  = [ems.mean, ems.variance, ems.std, ems.skew, ems.kurtosis]\n","                    results, _, _   = gmm.match_moments(target_moments)\n","                    gmm_scatter     = gmm.gmm_pdf(x, results)\n","                    gmm_ems         = mfl.Moments(x, gmm_scatter)\n","                    gmm_vals        = np.concatenate((vals, results))\n","                    gmm_array[idx]  = np.concatenate((gmm_vals, gmm_ems.moments))\n","                    idx += 1\n","                    if idx % 1000 == 0:\n","                        print(idx)\n","\n","database    = pd.DataFrame(data_array, columns=cols)\n","database2   = pd.DataFrame(gmm_array, columns=gmm_names)\n","end         = time.time()\n","print(\"Time taken: \", (end - start)/60)\n","\n","database.to_csv('delta_database.csv', index=False)\n","database2.to_csv('gmm_database.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"LD2b3ywaNCxb"},"source":["##### clean up gmm database\n","\n","GMM database may not have perfectly fit to every delta scatterer. these records should be removed in the case that they fall outside the parameter space of the delta dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7PtSWiw7WI5"},"outputs":[],"source":["db2['good']     = True\n","\n","SampleRate      = 100\n","ScatterLen      = 4\n","x               = np.round(np.linspace(-ScatterLen/2, ScatterLen/2, ScatterLen*SampleRate, endpoint=False), 3)\n","\n","for idx, row in db2.iterrows():\n","    if idx % 1000 == 0:\n","        print(idx)\n","    gmm_scatter = sfl.GMM(x, row['mu1'], row['mu2'], row['sigma'], row['w1'])\n","    mnts        = mfl.Moments(gmm_scatter.x, gmm_scatter.scatter)\n","    mnts_short  = [mnts.mean, mnts.std, mnts.skew, mnts.kurtosis]\n","\n","    if (\n","        (np.round(mnts.mean, 2) != np.round(row['mean'], 2))\n","        or (np.round(mnts.std, 2) != np.round(row['std'], 2))\n","        or (np.round(mnts.skew, 2) != np.round(row['skew'], 2))\n","        or (np.round(mnts.kurtosis, 2) != np.round(row['kurtosis'], 2))\n","        or (np.round(db2.loc[idx, 'mean'], 2) != np.round(db.loc[idx, 'mean'], 2))\n","        or (np.round(db2.loc[idx, 'std'], 2) != np.round(db.loc[idx, 'std'], 2))\n","        or (np.round(db2.loc[idx, 'skew'], 2) != np.round(db.loc[idx, 'skew'], 2))\n","        or (np.round(db2.loc[idx, 'kurtosis'], 2) != np.round(db.loc[idx, 'kurtosis'], 2))\n","    ):\n","        db2.loc[idx, 'good'] = False\n","\n","db2     = db2[db2['good'] == True]\n","\n","print(db.shape)\n","print(db2.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeFZOXG-9Gc6"},"outputs":[],"source":["db2.to_csv('gmm_database.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"fj4Gb3r7K3pM"},"source":["# Polynomial Fits\n","(and new, complete database for ML training)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfVpMKB1LCTX"},"outputs":[],"source":["db      = pd.read_csv('delta_database.csv')\n","db2     = pd.read_csv('gmm_database.csv')"]},{"cell_type":"markdown","metadata":{"id":"0HJP4wE14viA"},"source":["### reports (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CFRnVRblaBOK"},"outputs":[],"source":["plt.rcParams['figure.figsize'] = [4, 2]\n","plt.rcParams['figure.dpi'] = 100\n","\n","plt.hist(db['mean'], bins=100)\n","plt.show()\n","\n","plt.hist(db['std'], bins=100)\n","plt.show()\n","\n","plt.hist(db['variance'], bins=100)\n","plt.show()\n","\n","plt.hist(db['skew'], bins=100)\n","plt.show()\n","\n","plt.hist(db['kurtosis'], bins=100)\n","plt.show()\n","\n","plt.hist(1/db['kurtosis'], bins=100)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BMqyvk1-fI9d"},"source":["## tests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtcwB3OODog2"},"outputs":[],"source":["plt.rcParams['figure.dpi'] = 400\n","fig, axs = plt.subplots(2, 1, figsize=(4, 4))\n","\n","# axs[0].plot(sr1.t, np.abs(sr1.signal), label='abs')\n","axs[0].plot(sr1.t, np.real(sr1.signal), label='real')\n","axs[0].plot(sr1.t, np.imag(sr1.signal), label='imag')\n","axs[0].set_xlim(-2.5, 2.5)\n","axs[0].set_ylim(-6.2, 5)\n","axs[0].legend(loc='lower right')\n","axs[0].set_xlabel(r'Time (units of $B^{-1}$)')\n","axs[0].vlines([-1.5, -0.5, 0.5, 1.5], -6.5, 5.5, linestyles='dashed', colors='r')\n","axs[0].text(-2.1, 3.5, r'$t_1$', fontsize=12)\n","axs[0].text(-1.1, 3.5, r'$t_2$', fontsize=12)\n","axs[0].text(-0.1, 3.5, r'$t_3$', fontsize=12)\n","axs[0].text(0.9, 3.5, r'$t_4$', fontsize=12)\n","axs[0].text(1.9, 3.5, r'$t_5$', fontsize=12)\n","\n","ft = np.fft.fftfreq(len(sr1.signal), 1/SampleRate)\n","fts = np.fft.fftshift(ft)\n","ffts = np.fft.fftshift(np.real(np.fft.fft(sr1.signal)))\n","\n","axs[1].plot(fts, ffts/np.max(np.abs(ffts)))\n","axs[1].set_xlim(-.5, 1.5)\n","axs[1].set_xlabel('Frequency (units of $B$)')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TKbxnoyEfPJf"},"source":["## final database creation"]},{"cell_type":"markdown","metadata":{"id":"cIb2j-PEyCNj"},"source":["### base functions and variable dump"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0vFRQUDwVfO"},"outputs":[],"source":["# must run this cell every time you create a new final database\n","\n","power           = 4\n","snr             = 0\n","segments        = 5\n","chunk_size      = 1000\n","real            = False\n","add_noise       = False if (snr == 0) else True\n","bw              = 2*np.pi\n","SampleRate      = 100\n","SignalLen       = 10\n","ScatterLen      = 2\n","t               = np.round(np.linspace(-SignalLen/2, SignalLen/2, SignalLen*SampleRate, endpoint=False), 3)\n","x               = np.round(np.linspace(-ScatterLen/2, ScatterLen/2, ScatterLen*SampleRate, endpoint=False), 3)\n","poly_columns    = []\n","coefs           = [1, 1, -1, -1, -1, -1]\n","f_low           = 0\n","f_high          = f_low + 1\n","sr              = bwl.SuperRandom(f_low=f_low*bw, f_high=f_high*bw, t=t, coefs=coefs)\n","use_base        = False\n","\n","for seg in range(segments):\n","    for coef in range((power+1)):\n","        poly_columns.append(f'segment{seg}_{coef}_real')\n","for seg in range(segments):\n","    for coef in range((power+1)):\n","        poly_columns.append(f'segment{seg}_{coef}_imag')\n","\n","corr_columns    = ['0_lag_corr_base', 'best_corr_base', 'best_lag_base']"]},{"cell_type":"markdown","metadata":{"id":"br7oZaZOrrJt"},"source":["### delta version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZqeqIPLs8KUj"},"outputs":[],"source":["for snr in [10, 100]:\n","    add_noise       = False if (snr == 0) else True\n","    moment_columns  = db.columns.tolist()\n","    all_columns     = moment_columns + poly_columns + corr_columns\n","    output_file     = f'db_delta_snr{snr}_complex_{f_low}to{f_high}.csv'\n","\n","    if os.path.exists(output_file):\n","        processed_df = pd.read_csv(output_file)\n","        start_idx = len(processed_df)\n","        print(f\"Resuming from record {start_idx}\")\n","    else:\n","        start_idx = 0\n","        print(\"Starting processing from the beginning\")\n","\n","    st = time.time()\n","\n","    # Process in chunks to deal with running out of processing time\n","    for start in range(start_idx, len(db), chunk_size):\n","        end         = min(start + chunk_size, len(db))\n","        chunk       = db.iloc[start:end].copy()\n","\n","        print(f\"Processing records {start} to {end - 1}\")\n","\n","        start_time  = time.time()\n","        chunk_df    = chunk.apply(lambda row: pfl.add_poly_coefs(row, x, sr, power, add_noise=add_noise,\n","                                                                snr=snr, segments=segments, real=real, f_low=f_low*bw,\n","                                                                SampleRate=SampleRate, use_base=use_base),\n","                                axis=1).tolist()\n","        chunk_df    = pd.DataFrame(chunk_df, columns=all_columns)\n","\n","        if os.path.exists(output_file):\n","            chunk_df.to_csv(output_file, mode='a', header=False, index=False)\n","        else:\n","            chunk_df.to_csv(output_file, mode='w', header=True, index=False)\n","\n","        end_time = time.time()\n","\n","        print((end_time - start_time)/60)\n","\n","    et = time.time()\n","    print(\"Processing complete!\")\n","    print(\"Time taken: \", (et - st)/60)"]},{"cell_type":"markdown","metadata":{"id":"s0kfKiUSrtbl"},"source":["### GMM version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QO3eO8_uftTF"},"outputs":[],"source":["for snr in [10, 100]:\n","    add_noise       = False if (snr == 0) else True\n","    moment_columns2 = ['mu1', 'mu2', 'sigma', 'w1', 'mean', 'variance', 'std', 'skew',\n","                    'kurtosis', 'inv_kurt', 'hyperskewness', 'hyperkurtosis']\n","    corr_columns2    = ['0_lag_corr_delta', 'best_corr_delta', 'best_lag_delta']\n","    all_columns     = moment_columns2 + poly_columns + corr_columns + corr_columns2\n","    output_file     = f'db_gmm_snr{snr}_complex_{f_low}to{f_high}.csv'\n","\n","    if os.path.exists(output_file):\n","        processed_df = pd.read_csv(output_file)\n","        start_idx = len(processed_df)\n","        print(f\"Resuming from record {start_idx}\")\n","    else:\n","        start_idx = 0\n","        print(\"Starting processing from the beginning\")\n","\n","    # Process in chunks to deal with running out of processing time\n","    for start in range(start_idx, len(db2), chunk_size):\n","        end             = min(start + chunk_size, len(db2))\n","        chunk           = db2.iloc[start:end].copy()\n","\n","        print(f\"Processing records {start} to {end - 1}\")\n","\n","        start_time      = time.time()\n","        processed_chunk = chunk.apply(lambda row: pfl.add_poly_coefs_gmm(row, x, sr, power, add_noise=add_noise, snr=snr,\n","                                                                        segments=segments, real=real, f_low=f_low*bw,\n","                                                                        SampleRate=SampleRate, use_base=use_base),\n","                                    axis=1).tolist()\n","        chunk_df        = pd.DataFrame(processed_chunk, columns=all_columns)\n","\n","        if os.path.exists(output_file):\n","            chunk_df.to_csv(output_file, mode='a', header=False, index=False)\n","        else:\n","            chunk_df.to_csv(output_file, mode='w', header=True, index=False)\n","\n","        end_time        = time.time()\n","\n","        print((end_time - start_time)/60)\n","\n","    print(\"Processing complete!\")"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1I8YnOgTUz8lVxNpJcTyOSSJnlUuPQPqe","timestamp":1740525302259}],"authorship_tag":"ABX9TyMQNzx69W8H4lCZfaMmPBKQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}