{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uYpdtDP2NUNe"},"outputs":[],"source":["import torch\n","import joblib\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import TensorDataset, DataLoader, random_split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41673,"status":"ok","timestamp":1760378470414,"user":{"displayName":"Derek White","userId":"02576239188976074170"},"user_tz":420},"id":"vG80lHWcNcdW","outputId":"fc1cd6c6-38da-400b-8b63-09ff860b5649"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"hvWlzb6r_Olz"},"source":["# Core Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSS48wd8-k1m"},"outputs":[],"source":["# Define MLP model\n","class MLPRegressor(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, output_dim)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# training loop\n","def train(model, train_loader, val_loader, epochs=1000, schedule=True):\n","    train_loss_history = []\n","    val_loss_history = []\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss  = 0.0\n","        for X_batch, y_batch in train_loader:\n","            optimizer.zero_grad()\n","            output  = model(X_batch)\n","            loss    = loss_fn(output, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * X_batch.size(0)\n","\n","        avg_loss    = train_loss / len(train_loader.dataset)\n","\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                preds   = model(xb)\n","                loss    = loss_fn(preds, yb)\n","                val_loss += loss.item() * xb.size(0)\n","\n","        avg_train_loss  = train_loss / train_size\n","        avg_val_loss    = val_loss / val_size\n","\n","        train_loss_history.append(avg_train_loss)\n","        val_loss_history.append(avg_val_loss)\n","\n","        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","        # Optional: Reduce LR or stop early\n","        if schedule:\n","            scheduler.step(avg_train_loss)\n","\n","    return train_loss_history, val_loss_history\n","\n","# get dataset ready to be used in pytorch training loop\n","def prepare_data(db, inds, deps, input_scaler=None, output_scaler=None):\n","    X           = db[inds].values\n","    y           = db[deps].values\n","\n","    if input_scaler is None:\n","        input_scaler    = StandardScaler()\n","        X_scaled        = input_scaler.fit_transform(X)\n","    else:\n","        X_scaled        = input_scaler.transform(X)\n","\n","    if output_scaler is None:\n","        output_scaler   = StandardScaler()\n","        y_scaled        = output_scaler.fit_transform(y)\n","    else:\n","        y_scaled        = output_scaler.transform(y)\n","\n","    X_tensor    = torch.tensor(X_scaled, dtype=torch.float32)\n","    y_tensor    = torch.tensor(y_scaled, dtype=torch.float32)\n","    dataset     = TensorDataset(X_tensor, y_tensor)\n","    return dataset, input_scaler, output_scaler\n","\n","# make predictions on dataset based on trained model\n","def make_predictions(model, dataset, input_scaler, output_scaler):\n","    all_ins         = []\n","    all_preds       = []\n","    all_targets     = []\n","    dataset_loader  = DataLoader(dataset, batch_size=1024)\n","    with torch.no_grad():\n","        for xb, yb in dataset_loader:\n","            preds_scaled    = model(xb)\n","            preds           = output_scaler.inverse_transform(preds_scaled.numpy())\n","            targets         = output_scaler.inverse_transform(yb.numpy())\n","\n","            all_preds.append(preds)\n","            all_targets.append(targets)\n","    all_preds       = np.vstack(all_preds)\n","    all_targets     = np.vstack(all_targets)\n","    return all_targets, all_preds\n","\n","# quick function to select datasets based on saved file name\n","def choose_datasets(folder, snr, bw_start, use_base=False):\n","    db          = pd.read_csv(f'db_delta_snr{snr}_complex_{bw_start}to{bw_start+1}.csv')\n","    db2         = pd.read_csv(f'db_gmm_snr{snr}_complex_{bw_start}to{bw_start+1}.csv')\n","    return db, db2\n","\n","# create arrays of names of inputs and outputs for training\n","def create_training_columns(segments):\n","    cols = []\n","    for seg in range(segments):\n","        for coef in range((5)):\n","            cols.append(f'segment{seg}_{coef}_real')\n","            cols.append(f'segment{seg}_{coef}_imag')\n","    independents    = cols.copy()\n","    dependents      = ['mean', 'std', 'skew', 'inv_kurt']\n","    return independents, dependents"]},{"cell_type":"markdown","metadata":{"id":"pueSozxg-AgD"},"source":["# Train Network on Data"]},{"cell_type":"markdown","metadata":{"id":"csoOMChj9uBK"},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11027,"status":"ok","timestamp":1760379749460,"user":{"displayName":"Derek White","userId":"02576239188976074170"},"user_tz":420},"id":"tuV2vSVafud7","outputId":"818a01ad-ad6a-48ee-8e04-0cb006543c6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["687800\n"]}],"source":["foler       = '/your/folder/here'\n","snr         = 0         # 0, 10, or 100. SNR measured in average amplitude\n","run_num     = 0         # if you want to collect results from multiple training runs\n","bw_start    = 0         # 0 or 1, depending on interrogating wave's frequency band (default 0)\n","db, db2     = choose_datasets(folder, snr, run_num, bw_start)"]},{"cell_type":"markdown","metadata":{"id":"7HqfR706-Gj6"},"source":["## select which data to include as independent and dependent variables"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1760379749481,"user":{"displayName":"Derek White","userId":"02576239188976074170"},"user_tz":420},"id":"9VHmIAjUWJ7c","outputId":"8c962807-e45e-45dc-bd83-1cddfc034451"},"outputs":[{"output_type":"stream","name":"stdout","text":["['segment0_0_real', 'segment0_0_imag', 'segment0_1_real', 'segment0_1_imag', 'segment0_2_real', 'segment0_2_imag', 'segment0_3_real', 'segment0_3_imag', 'segment0_4_real', 'segment0_4_imag', 'segment1_0_real', 'segment1_0_imag', 'segment1_1_real', 'segment1_1_imag', 'segment1_2_real', 'segment1_2_imag', 'segment1_3_real', 'segment1_3_imag', 'segment1_4_real', 'segment1_4_imag', 'segment2_0_real', 'segment2_0_imag', 'segment2_1_real', 'segment2_1_imag', 'segment2_2_real', 'segment2_2_imag', 'segment2_3_real', 'segment2_3_imag', 'segment2_4_real', 'segment2_4_imag', 'segment3_0_real', 'segment3_0_imag', 'segment3_1_real', 'segment3_1_imag', 'segment3_2_real', 'segment3_2_imag', 'segment3_3_real', 'segment3_3_imag', 'segment3_4_real', 'segment3_4_imag', 'segment4_0_real', 'segment4_0_imag', 'segment4_1_real', 'segment4_1_imag', 'segment4_2_real', 'segment4_2_imag', 'segment4_3_real', 'segment4_3_imag', 'segment4_4_real', 'segment4_4_imag']\n","['mean', 'std', 'skew', 'inv_kurt']\n"]}],"source":["segments                    = 5\n","independents, dependents    = create_training_columns(segments, run_num)\n","\n","print(independents)\n","print(dependents)"]},{"cell_type":"markdown","metadata":{"id":"RWwK6f2b_jLA"},"source":["## run training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktNRjZJb_Ahw"},"outputs":[],"source":["dataset, _in, _out  = prepare_data(db, independents, dependents)\n","model               = MLPRegressor(input_dim=len(independents), output_dim=len(dependents))\n","loss_fn             = nn.HuberLoss()\n","optimizer           = optim.Adam(model.parameters(), lr=8e-4)\n","scheduler           = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=10)\n","val_size            = int(0.2 * len(dataset))\n","train_size          = len(dataset) - val_size\n","_train, _val        = random_split(dataset, [train_size, val_size])\n","train_loader        = DataLoader(_train, batch_size=512, shuffle=True)\n","val_loader          = DataLoader(_val, batch_size=512)\n","tl_hist, vl_hist    = train(model, train_loader, val_loader, epochs=500, schedule=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"j2tQGc0paY3Y"},"outputs":[],"source":["plt.plot(vl_hist, label='Validation Loss')\n","plt.plot(tl_hist, label='Training Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.yscale('log')\n","plt.legend()\n","\n","# save plot\n","plt.savefig(folder + f'plots/loss_snr{snr}_complex_{bw_start}to{bw_start+1}_run{run_num}.png')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7Ks8cNveyQ_-"},"source":["## save trained model and scalers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8dVcbHjvyTOT"},"outputs":[],"source":["sub_folder = folder + 'Predictions/'\n","torch.save(model, sub_folder + f'models/polys_snr{snr}_complex_{bw_start}to{bw_start+1}_run{run_num}.pt')\n","joblib.dump(_in, sub_folder + f'pickles/input_scaler_snr{snr}_{bw_start}to{bw_start+1}_run{run_num}.pkl')\n","joblib.dump(_out, sub_folder + f'pickles/output_scaler_snr{snr}_{bw_start}to{bw_start+1}_run{run_num}.pkl')"]},{"cell_type":"markdown","metadata":{"id":"vrb7l63Q_mUJ"},"source":["# Evaluate Results"]},{"cell_type":"markdown","metadata":{"id":"HZx9cOwPSnwx"},"source":["### do this stuff\n","\n","if you didn't just train the network and set all these variables already"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QyCSORtL37XG"},"outputs":[],"source":["sub_folder = folder + '/Predictions/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1aWDBsyybZ-"},"outputs":[],"source":["snr         = 0\n","run_num     = 0\n","bw_start    = 0\n","model       = torch.load(sub_folder + f'models/polys_snr{snr}_complex_{bw_start}to{bw_start+1}_run{run_num}.pt',\n","                         weights_only=False)\n","db, db2     = choose_datasets(folder, snr, run_num, bw_start)\n","\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxO3MCa6S-Hj"},"outputs":[],"source":["_in     = joblib.load(sub_folder + f'pickles/input_scaler_snr{snr}_{bw_start}to{bw_start+1}_run{run_num}.pkl')\n","_out    = joblib.load(sub_folder + f'pickles/output_scaler_snr{snr}_{bw_start}to{bw_start+1}_run{run_num}.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TCLaRD6zPF9"},"outputs":[],"source":["segments                    = 5\n","independents, dependents    = create_training_columns(segments, run_num)"]},{"cell_type":"markdown","metadata":{"id":"HYazUJo_TIHC"},"source":["### make predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nsOfIcR_GzS"},"outputs":[],"source":["scattertype     = 'delta'\n","\n","print(\"scatter type:\", scattertype)\n","print('')\n","\n","if scattertype == 'delta':\n","    dataset, _, _   = prepare_data(db, independents, dependents, _in, _out)\n","elif scattertype == 'gmm':\n","    dataset, _, _   = prepare_data(db2, independents, dependents, _in, _out)\n","\n","targets, preds  = make_predictions(model, dataset, _in, _out)\n","\n","rmse            = np.sqrt(np.mean((preds - targets)**2, axis=0))\n","mae             = np.mean(np.abs(preds - targets), axis=0)\n","\n","for name, error in zip(dependents, rmse):\n","    print(f\"RMSE for {name}: {error:.4f}\")\n","print('')\n","for name, error in zip(dependents, mae):\n","    print(f\"MAE for {name}: {error:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"porUwDanMYR7"},"source":["### save results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqDuMZF0MaIs"},"outputs":[],"source":["db_preds    = pd.DataFrame(preds, columns=[d + '_pred' for d in dependents])\n","\n","if scattertype == 'delta':\n","    db_w_preds  = pd.concat([db, db_preds], axis=1)\n","    filename = f'db_delta_snr{snr}_complex_w_preds_{bw_start}to{bw_start+1}_{run_num}.csv'\n","elif scattertype == 'gmm':\n","    db_w_preds  = pd.concat([db2, db_preds], axis=1)\n","    filename = f'db_gmm_snr{snr}_complex_w_preds_{bw_start}to{bw_start+1}_{run_num}.csv'\n","\n","db_w_preds.to_csv(sub_folder + filename, index=False)"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V5E1","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyO4Zf8TbpZXfGTzkHkCWiCd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}